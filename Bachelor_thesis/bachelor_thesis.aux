\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@sortscheme{none}
\abx@aux@sortnamekeyscheme{global}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@cite{DalfString}
\abx@aux@cite{carleoSolvingQuantumManybody2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{9}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:intro}{{1}{9}{Introduction}{chapter.1}{}}
\newlabel{chap:intro@cref}{{[chapter][1][]1}{9}}
\abx@aux@cite{Stringari}
\abx@aux@cite{pethick_smith_2008}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Bose-Einstein condensate}{10}{section.1.1}}
\abx@aux@cite{vmcarticle}
\abx@aux@cite{Giorgini}
\abx@aux@cite{Gruter}
\newlabel{GP}{{1.1}{11}{Bose-Einstein condensate}{equation.1.1.1}{}}
\newlabel{GP@cref}{{[equation][1][1]1.1}{11}}
\@writefile{toc}{\contentsline {part}{I\hspace  {1em}Implementation}{13}{part.1}}
\newlabel{part:implementation}{{I}{15}{Implementation}{part.1}{}}
\newlabel{part:implementation@cref}{{[part][1][]I}{15}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}The System}{15}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:system}{{2}{15}{The System}{chapter.2}{}}
\newlabel{chap:system@cref}{{[chapter][2][]2}{15}}
\newlabel{eq_hamilton}{{2.1}{15}{The System}{equation.2.0.1}{}}
\newlabel{eq_hamilton@cref}{{[equation][1][2]2.1}{15}}
\@writefile{toc}{\contentsline {paragraph}{No interaction - spherical trap}{15}{section*.1}}
\newlabel{analitica}{{2}{15}{No interaction - spherical trap}{section*.1}{}}
\newlabel{analitica@cref}{{[chapter][2][]2}{15}}
\@writefile{toc}{\contentsline {subparagraph}{No interaction - elliptical trap}{16}{section*.2}}
\newlabel{ham}{{2.2}{16}{No interaction - elliptical trap}{equation.2.0.2}{}}
\newlabel{ham@cref}{{[equation][2][2]2.2}{16}}
\@writefile{toc}{\contentsline {subparagraph}{Interaction - elliptical trap}{16}{section*.3}}
\newlabel{ham2}{{2.3}{16}{Interaction - elliptical trap}{equation.2.0.3}{}}
\newlabel{ham2@cref}{{[equation][3][2]2.3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Methods}{16}{section.2.1}}
\newlabel{var_princ}{{2.4}{16}{Methods}{equation.2.1.4}{}}
\newlabel{var_princ@cref}{{[equation][4][2]2.4}{16}}
\abx@aux@cite{morten_book}
\newlabel{PDF}{{2.5}{17}{Methods}{equation.2.1.5}{}}
\newlabel{PDF@cref}{{[equation][5][2]2.5}{17}}
\newlabel{local_energy}{{2.6}{17}{Methods}{equation.2.1.6}{}}
\newlabel{local_energy@cref}{{[equation][6][2]2.6}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Sampling methods}{17}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Brute-Force Metropolis algorithm}{18}{section*.4}}
\newlabel{eq_accept_move}{{2.7}{18}{Brute-Force Metropolis algorithm}{equation.2.1.7}{}}
\newlabel{eq_accept_move@cref}{{[equation][7][2]2.7}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Importance Sampling Metropolis-Hastings algorithm}{18}{section*.5}}
\newlabel{eq_imp_sam}{{2.8}{19}{Importance Sampling Metropolis-Hastings algorithm}{equation.2.1.8}{}}
\newlabel{eq_imp_sam@cref}{{[equation][8][2]2.8}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Iterative methods to find optimal variational parameters}{19}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent (SGD)}{19}{section*.6}}
\newlabel{sec_SGD}{{2.1.2}{19}{Stochastic Gradient Descent (SGD)}{section*.6}{}}
\newlabel{sec_SGD@cref}{{[subsection][2][2,1]2.1.2}{19}}
\newlabel{parameter}{{2.9}{19}{Stochastic Gradient Descent (SGD)}{equation.2.1.9}{}}
\newlabel{parameter@cref}{{[equation][9][2]2.9}{19}}
\newlabel{eq_energy_gradient}{{2.10}{19}{Stochastic Gradient Descent (SGD)}{equation.2.1.10}{}}
\newlabel{eq_energy_gradient@cref}{{[equation][10][2]2.10}{19}}
\newlabel{eq_grad_ak}{{2.1.2}{19}{Stochastic Gradient Descent (SGD)}{equation.2.1.10}{}}
\newlabel{eq_grad_ak@cref}{{[subsection][2][2,1]2.1.2}{19}}
\abx@aux@cite{marius}
\newlabel{eq_grad_bk}{{2.1.2}{20}{Stochastic Gradient Descent (SGD)}{equation.2.1.10}{}}
\newlabel{eq_grad_bk@cref}{{[subsection][2][2,1]2.1.2}{20}}
\newlabel{eq_grad_wk}{{2.1.2}{20}{Stochastic Gradient Descent (SGD)}{equation.2.1.10}{}}
\newlabel{eq_grad_wk@cref}{{[subsection][2][2,1]2.1.2}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Statistical analysis - blocking method}{20}{subsection.2.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Structure of the code}{21}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Trial Wave-function}{21}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}ML trial wave-function}{21}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Sketch of a Neural Network Quantum State: it shows the architecture of a Restricted Boltzmann Machine applied to our quantum many-body problem. The visible layer ($X_i$, with $i=1,\dots  ,$N) is a set of N visible neurons which take as input the positions of the particles, whereas the hidden layer ($H_j$, with $j=1,\dots  ,$M) is a set of M invisible neurons.\relax }}{22}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{Fig:draw}{{2.1}{22}{Sketch of a Neural Network Quantum State: it shows the architecture of a Restricted Boltzmann Machine applied to our quantum many-body problem. The visible layer ($X_i$, with $i=1,\dots ,$N) is a set of N visible neurons which take as input the positions of the particles, whereas the hidden layer ($H_j$, with $j=1,\dots ,$M) is a set of M invisible neurons.\relax }{figure.caption.7}{}}
\newlabel{Fig:draw@cref}{{[figure][1][2]2.1}{22}}
\newlabel{eq_rbm}{{2.2.1}{22}{ML trial wave-function}{figure.caption.7}{}}
\newlabel{eq_rbm@cref}{{[subsection][1][2,2]2.2.1}{22}}
\newlabel{eq_psi}{{2.2.1}{23}{ML trial wave-function}{figure.caption.7}{}}
\newlabel{eq_psi@cref}{{[subsection][1][2,2]2.2.1}{23}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis of Hamiltonian}{23}{section*.8}}
\newlabel{eq_loc_e}{{2.2.1}{23}{Analysis of Hamiltonian}{section*.8}{}}
\newlabel{eq_loc_e@cref}{{[subsection][1][2,2]2.2.1}{23}}
\newlabel{eq_gradient}{{2.11}{23}{Analysis of Hamiltonian}{equation.2.2.11}{}}
\newlabel{eq_gradient@cref}{{[equation][11][2]2.11}{23}}
\newlabel{eq_laplace}{{2.2.1}{24}{Analysis of Hamiltonian}{equation.2.2.11}{}}
\newlabel{eq_laplace@cref}{{[subsection][1][2,2]2.2.1}{24}}
\@writefile{toc}{\contentsline {paragraph}{Quantum force}{24}{section*.9}}
\newlabel{eq_quantumforce}{{2.2.1}{24}{Quantum force}{section*.9}{}}
\newlabel{eq_quantumforce@cref}{{[subsection][1][2,2]2.2.1}{24}}
\@writefile{toc}{\contentsline {part}{II\hspace  {1em}Data Analysis and Results}{25}{part.2}}
\newlabel{part:results}{{II}{27}{Data Analysis and Results}{part.2}{}}
\newlabel{part:results@cref}{{[part][2][]II}{27}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Non-Interacting Bosons in a Spherical and Elliptical Harmonic Trap}{27}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Non-interacting bosons in a spherical harmonic trap}{27}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}BFM algorithm}{28}{subsection.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Local energy evolution calculated as the program adapts using Stochastic Gradient Descent for various Metropolis step lengths. Two particles in two dimensions with two hidden nodes and $\eta = 0.1$ are considered. We note how for $L\geq 0.5$ are quite stable at $E_L=2\ \hbar \omega _{ho}$.\relax }}{28}{figure.caption.10}}
\newlabel{Fig:1}{{3.1}{28}{Local energy evolution calculated as the program adapts using Stochastic Gradient Descent for various Metropolis step lengths. Two particles in two dimensions with two hidden nodes and $\eta = 0.1$ are considered. We note how for $L\geq 0.5$ are quite stable at $E_L=2\ \hbar \omega _{ho}$.\relax }{figure.caption.10}{}}
\newlabel{Fig:1@cref}{{[figure][1][3]3.1}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Acceptance ratio as function of BFM step length. The data are made using 2 particles in 2 dimensions with 2 hidden nodes.\relax }}{29}{table.caption.11}}
\newlabel{Tab:1}{{3.1}{29}{Acceptance ratio as function of BFM step length. The data are made using 2 particles in 2 dimensions with 2 hidden nodes.\relax }{table.caption.11}{}}
\newlabel{Tab:1@cref}{{[table][1][3]3.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Local energy evolution calculated as the program adapts using Stochastic Gradient Descent for various BFM step lengths. Two particles in two dimensions with two hidden nodes and $\eta = 0.1$ are considered. As we increase the BFM step length, the value of $\sigma $ as well as its fluctuation decrease with the exception of the first two step length.\relax }}{29}{figure.caption.12}}
\newlabel{Fig:2}{{3.2}{29}{Local energy evolution calculated as the program adapts using Stochastic Gradient Descent for various BFM step lengths. Two particles in two dimensions with two hidden nodes and $\eta = 0.1$ are considered. As we increase the BFM step length, the value of $\sigma $ as well as its fluctuation decrease with the exception of the first two step length.\relax }{figure.caption.12}{}}
\newlabel{Fig:2@cref}{{[figure][2][3]3.2}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Local energy evolution calculated with BFM algorithm as the program adapts using SGD for two particles in 2D. BFM step length 1.0 is used for all the runs. Smaller learning rates give slower convergence.\relax }}{30}{figure.caption.13}}
\newlabel{Fig:3}{{3.3}{30}{Local energy evolution calculated with BFM algorithm as the program adapts using SGD for two particles in 2D. BFM step length 1.0 is used for all the runs. Smaller learning rates give slower convergence.\relax }{figure.caption.13}{}}
\newlabel{Fig:3@cref}{{[figure][3][3]3.3}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}ISMH algorithm}{30}{subsection.3.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Local energy evolution calculated with ISMH algorithm as the program adapts using SGD for two particles in 2D. The data are made using $\eta = 0.1$ and 2 hidden nodes. The convergence time is very fast, and is almost independent of $\Delta t$. By using $\Delta t=0.5$ the process is faster than with the other time steps. All the time steps converge to the same energy. By using $\Delta t = 0.001$, the local energy fluctuates visibly for 100 cycles. It will therefore be favorable to use $\Delta t > 0.001$.\relax }}{31}{figure.caption.14}}
\newlabel{Fig:4}{{3.4}{31}{Local energy evolution calculated with ISMH algorithm as the program adapts using SGD for two particles in 2D. The data are made using $\eta = 0.1$ and 2 hidden nodes. The convergence time is very fast, and is almost independent of $\Delta t$. By using $\Delta t=0.5$ the process is faster than with the other time steps. All the time steps converge to the same energy. By using $\Delta t = 0.001$, the local energy fluctuates visibly for 100 cycles. It will therefore be favorable to use $\Delta t > 0.001$.\relax }{figure.caption.14}{}}
\newlabel{Fig:4@cref}{{[figure][4][3]3.4}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Logarithm of the standard deviation evolution calculated with ISMH algorithm as the program adapts using SGD for two particles in 2D. The data are made using $\eta = 0.1$. $\Delta t\leq 0.05$ use a long time to converge to the energy. It will therefore be favorable to use $\Delta t > 0.05$.\relax }}{32}{figure.caption.15}}
\newlabel{Fig:5}{{3.5}{32}{Logarithm of the standard deviation evolution calculated with ISMH algorithm as the program adapts using SGD for two particles in 2D. The data are made using $\eta = 0.1$. $\Delta t\leq 0.05$ use a long time to converge to the energy. It will therefore be favorable to use $\Delta t > 0.05$.\relax }{figure.caption.15}{}}
\newlabel{Fig:5@cref}{{[figure][5][3]3.5}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Acceptance ratio as function of ISMH time step. The data are made using 2 particles in 2 dimensions and 2 hidden nodes.\relax }}{32}{table.caption.16}}
\newlabel{Tab:2}{{3.2}{32}{Acceptance ratio as function of ISMH time step. The data are made using 2 particles in 2 dimensions and 2 hidden nodes.\relax }{table.caption.16}{}}
\newlabel{Tab:2@cref}{{[table][2][3]3.2}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Local energy evolution calculated with ISMH algorithm sampling as the program adapts using SGD for two particles in 2D. Importance Sampling time step 0.5 is used for all the runs. Even though all the learning rates converge, we note that $\eta =0.01$ converges really slowly.\relax }}{33}{figure.caption.17}}
\newlabel{Fig:6}{{3.6}{33}{Local energy evolution calculated with ISMH algorithm sampling as the program adapts using SGD for two particles in 2D. Importance Sampling time step 0.5 is used for all the runs. Even though all the learning rates converge, we note that $\eta =0.01$ converges really slowly.\relax }{figure.caption.17}{}}
\newlabel{Fig:6@cref}{{[figure][6][3]3.6}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Energy of non-interacting bosons in a spherical harmonic trap}{33}{subsection.3.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Local energy of non-interacting bosons in a spherical harmonic trap. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with BFM as well as ISMH algorithms. The setup consists on 300 SGD cycles of 300 000 MC steps. The final run to compute the energy is done with $10^7$ MC steps. The BFM step length is $L=1.0$, ISMH time step is $\Delta t = 0.5$. Both algorithms use $\sigma = 1.00$. The error is calculated using the blocking method.\relax }}{34}{table.caption.18}}
\newlabel{Tab:3}{{3.3}{34}{Local energy of non-interacting bosons in a spherical harmonic trap. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with BFM as well as ISMH algorithms. The setup consists on 300 SGD cycles of 300 000 MC steps. The final run to compute the energy is done with $10^7$ MC steps. The BFM step length is $L=1.0$, ISMH time step is $\Delta t = 0.5$. Both algorithms use $\sigma = 1.00$. The error is calculated using the blocking method.\relax }{table.caption.18}{}}
\newlabel{Tab:3@cref}{{[table][3][3]3.3}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Non-interacting bosons in an elliptical harmonic trap}{34}{section.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Local energy per particle $E_L/N_p$ of non-interacting bosons in an elliptical harmonic trap. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with ISMH algorithm. The setup consists on 300 SGD cycles of 300 000 MC steps. The final run to compute the energy is done with $10^7$ MC steps. The ISMH time step is $\Delta t = 0.5$ and $\sigma = 1.00$. The error is calculated using the blocking method.\relax }}{35}{table.caption.19}}
\newlabel{Tab:3.2}{{3.4}{35}{Local energy per particle $E_L/N_p$ of non-interacting bosons in an elliptical harmonic trap. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with ISMH algorithm. The setup consists on 300 SGD cycles of 300 000 MC steps. The final run to compute the energy is done with $10^7$ MC steps. The ISMH time step is $\Delta t = 0.5$ and $\sigma = 1.00$. The error is calculated using the blocking method.\relax }{table.caption.19}{}}
\newlabel{Tab:3.2@cref}{{[table][4][3]3.4}{35}}
\newlabel{new_trial_wf}{{3.2}{35}{Non-interacting bosons in an elliptical harmonic trap}{table.caption.19}{}}
\newlabel{new_trial_wf@cref}{{[section][2][3]3.2}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Local energy per particle $E_L/N_p$ of non-interacting bosons in an elliptical harmonic trap where $\Psi $ is fixed with $\xi _1$ and $\xi _2$. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with ISMH algorithm. The setup consists on 600 SGD cycles of 300 000 MC steps. The ISMH time step is $\Delta t = 0.5$ and $\sigma = 1.00$. The error is calculated using the blocking method.\relax }}{36}{table.caption.20}}
\newlabel{Tab:3.3}{{3.5}{36}{Local energy per particle $E_L/N_p$ of non-interacting bosons in an elliptical harmonic trap where $\Psi $ is fixed with $\xi _1$ and $\xi _2$. The quantity $N_p$ is the number of bosons, D is the dimensionality, M is the number of hidden nodes and $\eta $ is the learning rate. We show the analytical results and the ones obtained with ISMH algorithm. The setup consists on 600 SGD cycles of 300 000 MC steps. The ISMH time step is $\Delta t = 0.5$ and $\sigma = 1.00$. The error is calculated using the blocking method.\relax }{table.caption.20}{}}
\newlabel{Tab:3.3@cref}{{[table][5][3]3.5}{36}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Interacting Bosons in an Elliptical Harmonic Trap}{37}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces In this figure, we plot the evolution of ten particles in three dimensions as the program adapts with SGD. We use M $=8$ and $\eta =0.01$. As we can see the local energy converges quickly.\relax }}{38}{figure.caption.21}}
\newlabel{Fig:7_8}{{4.1}{38}{In this figure, we plot the evolution of ten particles in three dimensions as the program adapts with SGD. We use M $=8$ and $\eta =0.01$. As we can see the local energy converges quickly.\relax }{figure.caption.21}{}}
\newlabel{Fig:7_8@cref}{{[figure][1][4]4.1}{38}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Setup used with the various configurations and results obtained: we show the number of hidden nodes $M$, learning rate $\eta $, acceptance rate for each configuration. The learning rate remains constant $for N_p\geq 20$ as we increase the number of particles whereas M is increased each time. \relax }}{39}{table.caption.22}}
\newlabel{Tab:4}{{4.1}{39}{Setup used with the various configurations and results obtained: we show the number of hidden nodes $M$, learning rate $\eta $, acceptance rate for each configuration. The learning rate remains constant $for N_p\geq 20$ as we increase the number of particles whereas M is increased each time. \relax }{table.caption.22}{}}
\newlabel{Tab:4@cref}{{[table][1][4]4.1}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Comparison between ML data from our code (blue dots) and the GP results (red dots) in \cite {DalfString} in the range $[0,100]$ and $[0,500]$. The green triangle is the expected $E_L/N_p$ for $N_p=1$: in this situation we are still in the non-interacting case. The point is inserted to help the reader to follow the GP trend below $N_p=100$. ML data seem to underestimate the GP results for $N_p\geq 20$. This problem is probably related to the insufficient number of hidden nodes used.\relax }}{40}{figure.caption.23}}
\newlabel{Fig:9}{{4.2}{40}{Comparison between ML data from our code (blue dots) and the GP results (red dots) in \cite {DalfString} in the range $[0,100]$ and $[0,500]$. The green triangle is the expected $E_L/N_p$ for $N_p=1$: in this situation we are still in the non-interacting case. The point is inserted to help the reader to follow the GP trend below $N_p=100$. ML data seem to underestimate the GP results for $N_p\geq 20$. This problem is probably related to the insufficient number of hidden nodes used.\relax }{figure.caption.23}{}}
\newlabel{Fig:9@cref}{{[figure][2][4]4.2}{40}}
\@writefile{toc}{\contentsline {part}{III\hspace  {1em}Conclusions}{41}{part.3}}
\newlabel{part:conclusion}{{III}{43}{Conclusions}{part.3}{}}
\newlabel{part:conclusion@cref}{{[part][3][]III}{43}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Summary and Outlook}{43}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:conclusion}{{5}{43}{Summary and Outlook}{chapter.5}{}}
\newlabel{chap:conclusion@cref}{{[chapter][5][]5}{43}}
